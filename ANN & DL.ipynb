{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ce68a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes & Lecture by Akhona Njeje.\n",
    "# Date 10 June 2023.\n",
    "# Topic & Solution : Artificial Neural Networks, Understanding ANN & DL.\n",
    "\n",
    "\n",
    "# Theory:\n",
    "\n",
    "### Topics ---> Perceptron Model to Neural Networks, Activation Function, Cost Function, \n",
    "###             Feed Forward Networks & Back Propagation.\n",
    "\n",
    "### Percepton Model ---> This model is about a general understanding of how biological neurons work.\n",
    "###                      GOOGLE Translator uses NN. A Perceptron may eventually be able to learn , \n",
    "###                      make decisions & translate languages.\n",
    "###                      Percepton Models were created/discovered in 1958.\n",
    "\n",
    "### Biologicial Model ---> Uses Dendrites to input data on the Nucleus. Output is Axon.\n",
    "### ANN               ---> Percetron Model, contains Inputs & Outputs.\n",
    "###                        x1 & x2 = Intputs = Dendrites, f(x) = Neuron = Nucleus, y = Output = Axon.\n",
    "###                        Simplified version of the Bio model to ANN is y = x1 + x2.\n",
    "###                        y = sum(xi*wi) + bi, xi = Input, wi = weight, bi = bias.\n",
    "\n",
    "### ANN               ---> Perceptron M = is a single model, ANN = is a Multi layer of perceptron models.\n",
    "\n",
    "### How do ANN work?  ---> 1. 1st layer is the input layer.\n",
    "###                        2. Last layer is the output layer.\n",
    "###                        3. Hidden layers = All layers between 1st & last layers.\n",
    "###                        4. Hidden layer = Blackbox in ANN.\n",
    "###                        5. NN the lager they become they become Deep Neural Networks(DNN).\n",
    "\n",
    "# AI WINTER:\n",
    "\n",
    "### The feild of Research in AI & NN in the 1970's got little to no attention & no funding went to it.\n",
    "### Reason being the Computation Power at the time, was not powerful enough to give us info on perceptrons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3019a9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Functions.\n",
    "\n",
    "### z = xw + b, activiation function is f(Z).\n",
    "### f(Z) = Sigmoid Function aka Logistics Regression model.[0,1].\n",
    "### f(z) = Tanhx, Sinhx & Coshx.[-1,1].\n",
    "### f(Z) = ReLU, f(x) = 0 for x < 0 & f(x) = x for x => 0.\n",
    "\n",
    "### Documentation & more info on Wikipedia(https://en.wikipedia.org/wiki/Activation_function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cb8177c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Class Classification Consideration.\n",
    "\n",
    "### Uses Softmax function.\n",
    "### Std(z)_i = exp(z_i) / sum(exp(z_j)) for i = 1,..., K.\n",
    "### This function computes all the probabilities of each target class over all possible target classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5436c5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost functions & Gradient descent.\n",
    "\n",
    "### How do we improve the perfomance of our models?\n",
    "### We use Cost Functions.\n",
    "\n",
    "### y = True value.\n",
    "### a = Neurons Prediction.\n",
    "### z = xw + b, Std(z) = a.\n",
    "\n",
    "### Cost = sum(y(x) - a(x))^2 / 2n, y(x) is the true value & a(x) is Neurons prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9336afcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent.\n",
    "\n",
    "### Calculating multiple tangents on Cost = C(W) = C(w1,...,wn). Its Gradient descent(Gd).\n",
    "### In search of finding C(W), Gd helps learn about the Learning Rate & find the minmum Cost we need.\n",
    "\n",
    "### Gd has multiple Algorithms, not just one. Adam, AdaDelta, AdaGrad, RMSProp etc all these are Gd Algorithms.\n",
    "### C'(W) = Gradient descent(Gd). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a65ce4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation.\n",
    "\n",
    "### How do we adjust our weights & biases? \n",
    "### We use Backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdcb55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow(TF) vs Keras(K).\n",
    "\n",
    "### TF is a Deep Learning library developed by Google.\n",
    "### TF has a large ecosystem & other libraries like Tensorboard, Deployment & Production API's.\n",
    "\n",
    "### K is a high level python library that can use a variety of DL libs such as Theano, CNTK & Tensorflow.\n",
    "\n",
    "### TF adopted K as the official API for TF."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
